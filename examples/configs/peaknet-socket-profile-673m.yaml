# PeakNet Pipeline Configuration with Socket Data Source
# Configuration for continuous streaming from LCLStreamer
#
# PROCESSING MODES:
# - Continuous (total_samples: null): Process data until socket timeout or Ctrl+C
# - Finite (total_samples: N): Process exactly N samples then stop

model:
  weights_path: /sdf/data/lcls/ds/prj/prjcwang31/results/proj-stream-to-ml/peaknet-673m.bin  # PeakNet-673M pretrained weights

  peaknet_config:
    model:
      # Basic model parameters
      image_size: 1920
      num_channels: 1
      num_classes: 2

      # Backbone configuration (PeakNet-673M)
      backbone_hidden_sizes: [352, 704, 1408, 2816]
      backbone_depths: [3, 3, 27, 3]

      # BiFPN configuration (PeakNet-673M)
      bifpn_num_blocks: 4
      bifpn_num_features: 512

      # Segmentation head configuration
      seg_out_channels: 256
      uses_learned_upsample: true  # Required for PeakNet-673M pretrained weights

      # Other settings
      from_scratch: false
  warmup_iterations: 100

runtime:
  max_actors: 4           # Multi-GPU profiling
  batch_size: 8          # Reasonable for large images
  num_producers: null        # Used in random mode only; ignored in socket mode (auto-inferred from socket_addresses)
  total_samples: null     # null = continuous processing until stream ends (recommended for socket sources)
                          # Set to positive integer for finite processing (e.g., 20480 for testing)
  inter_batch_delay: 0.01 # Moderate streaming rate
  queue_num_shards: 8
  queue_maxsize_per_shard: 1600
  # Coordination timing to eliminate pipeline gaps
  max_empty_polls: 500    # Wait longer before coordinator check (reduces gaps)
  poll_timeout: 0.001     # Faster polling for responsiveness (1ms)

data:
  shape: [1, 1920, 1920]  # Full resolution as requested (channels inferred from shape[0])

# Data source configuration for socket streaming
data_source:
  source_type: socket     # "socket" = stream from LCLStreamer, "random" = generate synthetic data

  # Multi-socket configuration (list of [hostname, port] pairs)
  # Single socket example:
  socket_addresses:
    - ["sdfada005", 12321]
    - ["sdfada005", 12322]

  socket_timeout: 300.0        # Socket timeout: wait this long for data before assuming stream ended
  socket_retry_attempts: 5    # Number of connection retry attempts

  # Serialization format
  serialization_format: numpy  # "numpy" for fast .npz format (recommended), "hdf5" for legacy format

  # Parsing location (NEW: for performance testing)
  parse_location: producer    # "consumer" = parse in Q1→P (default, GPU overlap)
                              # "producer" = parse in S→Q1 (alternative for testing)

  # NumPy field mapping (must match lclstreamer NumpyBinarySerializer keys)
  fields:
    detector_data: "detector_data"           # Main image data from detector
    timestamp: "timestamp"                   # Event timestamp
    photon_wavelength: "photon_wavelength"   # Photon wavelength/energy

  # Direct batch pass-through: trust pusher to provide appropriately sized batches
  batch_assembly: false       # Disable artificial batch assembly - use pusher's batches directly
  # batch_timeout: removed    # No timeout needed with direct pass-through

  # Required fields for validation
  required_fields: ["detector_data"]  # Fields that must be present in each .npz data packet

# NEW: Mixed precision configuration for inference performance
precision:
  dtype: "bfloat16"         # Precision type: "float32", "bfloat16", "float16"
                            # Autocast is automatically used for CUDA devices with bfloat16/float16

system:
  min_gpus: 1             # Minimum GPUs required
  skip_gpu_validation: false
  pin_memory: true        # Better GPU transfer performance
  verify_actors: false     # Verify GPU health

profiling:
  enable_profiling: true # Disable for initial testing
  output_dir: "./profiling_results"

output:
  output_dir: "./test_results"
  verbose: true           # Detailed output for analysis
  quiet: false

# Additional notes:
# 1. To use random data source instead of socket, set data_source.source_type to "random"
# 2. The socket_hostname should match the machine running LCLStreamer
# 3. The fields mapping should match the LCLStreamer NumpyBinarySerializer output
# 4. For testing, you can reduce total_samples to get faster feedback
# 5. NumPy .npz format is 3-7x faster than HDF5 for parsing (eliminates bottleneck)
#
# WARMUP BEHAVIOR:
# 6. Pipeline uses data.shape for immediate warmup, then waits for socket data
# 7. The pipeline will BLOCK until actual socket data arrives (no infinite retry warnings)
#
# SERIALIZATION FORMAT:
# 8. Use serialization_format: numpy with lclstreamer's NumpyBinarySerializer
# 9. For legacy HDF5 streams, set serialization_format: hdf5 and use hierarchical paths
#
# PARSING LOCATION (NEW):
# 10. parse_location: consumer (default) - Parse in actor during GPU overlap (Q1→P stage)
# 11. parse_location: producer - Parse in producer before queueing (S→Q1 stage)
# 12. Test both modes to compare throughput and determine optimal strategy for your workload
#
# Example usage:
# peaknet-pipeline --config /sdf/data/lcls/ds/prj/prjcwang31/results/proj-stream-to-ml/peaknet-socket-profile.yaml --verbose
#
# Or override from command line:
# peaknet-pipeline --config peaknet-socket-profile.yaml --socket-hostname sdfada015 --verbose
