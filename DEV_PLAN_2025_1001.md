# Development Plan - October 1, 2025

## Context

Successfully demonstrated socket-based producer-consumer pipeline working with real data streaming (tagged as `v0.1.0-socket-streaming`). This is a major milestone moving from random in-memory data to actual streaming data.

## Development Directions

### Direction 1: GPU Starvation & Scaling Analysis

**Objective:** Understand how data size (batch size and spatial dimensions) impacts GPU utilization and identify optimal producer-to-consumer scaling ratios.

#### Key Metrics to Track

- **GPU utilization**: Monitor via `nvidia-smi dmon` or nsys profiling (target >90% during inference)
- **Pipeline stage timings**:
  - H2D transfer time
  - Kernel execution time
  - D2H transfer time
  - Preprocessing overhead
- **Queue depths**: Monitor Q1 (input queue) and Q2 (output queue) sizes over time
- **Throughput measurements**:
  - Producer throughput (samples/sec)
  - Consumer throughput (samples/sec)
  - Throughput ratio
- **Idle time analysis**:
  - GPU waiting for data
  - Producer waiting for consumption

#### Experimental Approach

1. **Baseline profiling**: Start with current working config, profile with nsys to establish baseline timings
2. **Batch size sweep**: Vary batch size while holding spatial size constant → identify GPU saturation point
3. **Spatial size sweep**: Vary spatial dimensions → understand memory bandwidth bottlenecks
4. **Steady-state analysis**: Measure producer vs consumer rate ratio at steady state
5. **Producer scaling**: Scale producers incrementally, monitor for diminishing returns

#### Rule of Thumb

Producer throughput should be **~10-20% higher** than consumer throughput to keep GPUs fed without excessive queuing.

#### Key Questions to Answer

- Does GPU face input data starvation?
- How to parameterize the system scientifically?
- What's the optimal CPU (producer) to GPU (consumer) ratio?
- How to measure this ratio both rigorously and approximately?

---

### Direction 2: Object Store Spill Management

**Objective:** Understand and mitigate Ray object store spill issues when producer rate exceeds consumer rate.

#### Problem Statement

When data producing rate is higher than consumption rate, Ray's object store will spill to disk, causing **massive performance degradation**. This is a common problem in streaming systems.

#### Monitoring

- Use `ray memory --stats` to monitor object store usage
- Check Ray dashboard for spill events
- Monitor Q1 queue depth as early warning signal

#### Best Practices

1. **Backpressure mechanism**: Limit producer rate when Q1 reaches threshold
   - Prevents unbounded memory growth
   - Keeps data in memory (fast path)

2. **Right-size object store**: Configure at Ray initialization
   ```bash
   ray start --object-store-memory=<bytes>
   ```

3. **Bounded queues**: Use Ray actor mechanisms
   - `max_concurrency` to throttle producers
   - `max_pending_calls` for task submission limits

4. **Fast path optimization**: Evaluate if all data needs object store
   - Consider direct GPU memory transfers where possible

5. **Capacity planning**:
   - Size Q1 to handle bursts
   - Signal backpressure before spill threshold
   - Design for steady-state producer ≈ consumer rate

#### Pipeline-Specific Considerations

- If producers consistently outpace consumers → **backpressure is mandatory**
- Monitor spill events under various load conditions
- Consider producer throttling via Ray actor `max_concurrency`
- Q1 sizing: balance burst handling vs memory pressure

---

## Suggested Next Steps

### Phase 1: Instrumentation (Priority: High)
- [ ] Add metrics collection to pipeline:
  - Queue size tracking (Q1, Q2)
  - Throughput measurements (producer, consumer)
  - GPU utilization monitoring
  - Stage-level timing breakdowns
- [ ] Create metrics logging/visualization

### Phase 2: Scaling Experiments (Priority: High)
- [ ] Run batch size sweep experiments
- [ ] Run spatial size sweep experiments
- [ ] Profile with nsys at different scales
- [ ] Analyze GPU starvation patterns

### Phase 3: Object Store Analysis (Priority: Medium)
- [ ] Monitor object store usage under load
- [ ] Identify spill thresholds
- [ ] Measure performance degradation from spill

### Phase 4: Optimization (Priority: Medium)
- [ ] Implement basic backpressure mechanism
- [ ] Tune object store sizing
- [ ] Optimize producer-consumer ratio based on empirical data

### Phase 5: Documentation (Priority: Low)
- [ ] Document optimal configurations
- [ ] Create tuning guide based on experimental results
- [ ] Parameterize system for different workload profiles

---

## Open Questions

1. What is the current producer vs consumer throughput ratio?
2. At what queue depth should backpressure trigger?
3. What's the object store size needed for typical workloads?
4. Can we predict optimal scaling ratios from model characteristics?
5. Are there Ray patterns we're missing for streaming workloads?

---

## References

- Ray documentation: `/sdf/data/lcls/ds/prj/prjcwang31/results/codes/ray/doc/`
- Ray patterns: `/sdf/data/lcls/ds/prj/prjcwang31/results/learn-ray/PATTERNS.md`
- Current pipeline: `peaknet_pipeline_ray/`
